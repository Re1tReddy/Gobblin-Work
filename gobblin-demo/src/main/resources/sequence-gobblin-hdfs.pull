###################### job configuration file ######################

job.name=SequenceFiles_To_HDFS
job.group=Gobblin
job.description=Demo job to push the kafka data as sequence files to HDFS.
job.schedule=0/60 * * * * ?
job.commit.policy=full

kafka.brokers=localhost:9092
source.class=com.gobblin.sequence.hdfs.SequenceSource

#gobblin.util.limiter.CountBasedLimiter=10000

mapreduce.map.memory.mb=4095
mapreduce.map.java.opts=-Xmx3686m
mapreduce.job.queuename=jobs

#extract.limit.enabled=true
#extract.limit.type=count
#extract.limit.time.limit=15
#extract.limit.time.limit.timeunit=minutes
#extract.limit.count.limit=100000
extract.namespace=com.rasa.cloud.gobblin

topic.whitelist=test
#writer.partitioner.class=com.rasa.cloud.gobblin.SequencePartitioner
#writer.partition.granularity=hour
#writer.partition.pattern=YYYY-MM-dd-HH
#writer.partition.columns=ts
#writer.partition.timezone=UTC
#writer.partition.prefix=tid

writer.builder.class=com.gobblin.sequence.hdfs.SequenceDataWriterBuilder
writer.file.path.type=tablename
writer.destination.type=HDFS
writer.output.format=


#data.publisher.type=gobblin.publisher.TimePartitionedDataPublisher
data.publisher.replace.final.dir=false


mr.job.max.mappers=8

metrics.reporting.file.enabled=true
metrics.log.dir=/opt/gobblin/job_work/metrics
metrics.reporting.file.suffix=txt

bootstrap.with.offset=earliest

fs.uri=hdfs://localhost:9000
writer.fs.uri=hdfs://localhost:9000
state.store.fs.uri=hdfs://localhost:9000


mr.job.root.dir=/opt/gobblin/job_work/working
state.store.dir=/opt/gobblin/job_work/state-store
task.data.root.dir=/opt/gobblin/job_work/task-data
writer.staging.dir=/opt/gobblin/job_work/task-staging
writer.output.dir=/opt/gobblin/job_work/task-output
data.publisher.final.dir=/opt/gobblin/output

